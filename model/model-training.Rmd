---
title: "Model training"
output: html_notebook
---

This notebook trains a classification model which distinguishes between actual quotations to the biblical text and mere noise. It does not attempt to distinguish between versions of a biblical text: that kind of cleaning will happen later.

We are going to use the parsnip package and its attendants to train and evaluate different models, then pick the best one.

```{r setup, message=FALSE}
library(tidyverse)
library(tidymodels)
library(parsnip)
library(dials)
```

The training data is located in the database. It is stored as a table contained document (i.e., newspaper) and verse IDs, with a boolean labeling them as a genuine or false match. That match indicates whether the verse was indeed quoted, but not whether that specific version was quoted. Another table contains measurements of the features of the potential quotation. We join the labeled data to those measurements. But it might also be useful to know certain information about the version of the verse. For instance, the Book of Mormon reproduces a number of verses or phrases from the KJV and so it has a much higher rate of potential matches that measure highly but aren't actually matches. So we pull in some version information that we will manipulate. Finally we split the data into training and testing sets. The testing set is inviolable, and will be used for model validation later. To make sure the data is available for inspection later, we will only do that if the data has not been written to disk; otherwise, we will load the data from disk.

```{r}
# Check if we have already commited the training and validation data
if (!file.exists("apb-training.csv") |
    !file.exists("apb-testing.csv") |
    !file.exists("apb-labeled-quotations.csv")) {
  
  message("Reading the labels from the database and creating train/test split.\n")
  
  # Get the data from the database and manipulate it
  library(odbc)
  db <- dbConnect(odbc::odbc(), "Research DB")
  apb_labeled <- tbl(db, "apb_labeled")
  apb_potential_quotations <- tbl(db, "apb_potential_quotations")
  scriptures <- tbl(db, "scriptures") %>% select(verse_id = doc_id, version)
  labeled_quotations <- apb_labeled %>% 
    left_join(apb_potential_quotations, by = c("verse_id", "doc_id")) %>% 
    left_join(scriptures, by = c("verse_id")) %>% 
    collect() %>% 
    filter(!is.na(tokens)) %>% 
    mutate(match = if_else(match, "quotation", "noise"),
           match = factor(match, levels = c("quotation", "noise")),
           lds = if_else(version %in% c("Book of Mormon", 
                                        "Doctrine and Covenants",
                                        "Pearl of Great Price"),
                         "lds", "not-lds") %>% as.factor()) %>% 
    select(-version)
  
  # Split the labeled data into training and validation sets
  set.seed(1989)
  data_split <- initial_split(labeled_quotations, strata = "match", p = 0.85)
  training <- training(data_split)
  testing  <- testing(data_split)
  write_csv(labeled_quotations, "apb-labeled-quotations.csv")
  write_csv(training, "apb-training.csv")
  write_csv(testing, "apb-testing.csv")
  
  # Cleanup
  dbDisconnect(db)
  rm(data_split)
  rm(apb_labeled)
  rm(apb_potential_quotations)
  rm(scriptures)
  rm(db)
  
} else {
  
  message("The training or testing data already exists. Loading from disk.\n")
  spec <- cols(verse_id = col_character(),
               doc_id = col_character(),
               match = readr::col_factor(levels = c("quotation", "noise")),
               tokens = col_integer(),
               tfidf = col_double(),
               proportion = col_double(),
               runs_pval = col_double(), 
               lds = readr::col_factor(levels = c("lds", "not-lds")))
  labeled_quotations <- read_csv("apb-labeled-quotations.csv", col_types = spec)
  training <- read_csv("apb-training.csv", col_types = spec)
  testing <- read_csv("apb-testing.csv", col_types = spec)
  rm(spec)
  
}
```

We are going to remove the `verse_id` and `doc_id` columns because they are not predictor or response variables. And for NA values in the runs test, we substitute 1, because there was not enough matching data to compute the runs test. 

```{r}
labeled_quotations <- labeled_quotations %>% 
  mutate(runs_pval = if_else(is.na(runs_pval), 1, runs_pval)) %>% 
  select(-verse_id, -doc_id)
training <- training %>% 
  mutate(runs_pval = if_else(is.na(runs_pval), 1, runs_pval)) %>% 
  select(-verse_id, -doc_id)
testing <- testing %>% 
  mutate(runs_pval = if_else(is.na(runs_pval), 1, runs_pval)) %>% 
  select(-verse_id, -doc_id)
```

Some brief exploration of the data confirms that there is a clear separation in the data.

```{r}
labeled_quotations %>% 
  group_by(match) %>% 
  summarize(n(), mean(tokens), mean(tfidf), mean(proportion), mean(runs_pval)) %>% 
  gather("measurement", "value", -match) %>% 
  mutate(value = round(value, 2)) %>% 
  spread(match, value)
```

We can also see the separation in the data, thought it is not as clear as we would like.

```{r}
ggplot(labeled_quotations, aes(tokens, tfidf, color = match)) +
  geom_jitter(shape = 1) +
  theme_classic() +
  coord_cartesian(xlim = c(0, 100), ylim = c(0, 12)) +
  labs(title = "Comparison of genuine quotations versus noise")
```

We are going to pre-process the data to center and scale the predictors.

```{r}
data_recipe <- recipe(match ~ ., data = training) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_dummy(all_nominal(), -match) %>% 
  prep(training = training, retain = TRUE)

training_normalized = bake(data_recipe, newdata = training)
testing_normalized = bake(data_recipe, newdata = testing)
```

We will begin by training a logistic regression model to classify the quotations.

```{r}
set.seed(7260)
model_spec <- logistic_reg(mode = "classification")

model_fit <- model_spec %>% 
  parsnip::fit(match ~ .,
               data = training_normalized, 
               engine = "glm",
               control = fit_control(verbosity = 1))
```

We can then evaluate the accuracy of the model on the training dataset. (Later we will evaluate against the testing dataset.)

```{r}
training_results <- training_normalized %>% 
  select(match) %>%
  mutate(pred_class = model_fit %>% 
           predict_class(training_normalized),
         pred_probs = model_fit %>% 
           predict_classprob(training_normalized) %>% 
           pull(quotation))
training_results %>% accuracy(truth = match, estimate = pred_class)
training_results %>% roc_auc(truth = match, estimate = pred_probs)
training_results %>% pr_auc(truth = match, estimate = pred_probs)
training_results %>% conf_mat(truth = match, estimate = pred_class)
training_results %>% conf_mat(truth = match, estimate = pred_class) %>% summary()
training_results %>% roc_curve(match, pred_probs) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_classic() +
  labs(title = "ROC curve")
```

